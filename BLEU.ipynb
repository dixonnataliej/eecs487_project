{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "656I32mVWLpe",
        "outputId": "5372d92d-2ea1-43f6-a951-2820f952d274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import json\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('wolof.json', 'r') as fp:\n",
        "    wolof = json.load(fp)\n",
        "print(wolof['passages']['1']['official'])\n",
        "print(wolof['passages']['1']['chatgpt'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAgaLL7aWgRm",
        "outputId": "3945190a-c459-4a0c-852f-f6ca83f6f0e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ca njàlbéen ga Yàlla sàkk na asamaan ak suuf. Suuf nag daa maase woon te wéet, lëndëm muur ndox mu xóot mi, lale ca kaw, Noowug Yàlla di wër, tiim ndox mi.Ba loolu amee Yàlla ne: «Na leer nekk,» leer daldi am. Yàlla gis ne leer gi lu baax la, mu xàjjale leer ak lëndëm. Yàlla tudde leer gi bëccëg, lëndëm gi guddi. Ngoon jot, suba dugg, muy bés ba jëkk.\n",
            "Ci kawas, Yàlla sàkk ay asamaan ak suuf. Suuf bi dootul am benn doxaan, te nekkul dara; te lëndëm taxawoon na ci kaw géej gu réy. Te Xelmu Yàlla dafa yéegoon ci kaw ndox yi. Te Yàlla wax ne: \"Na leer am.\" Leer am na. Te Yàlla gis leer, ne baax na; mu deal leer ak lëndëm. Te Yàlla tudde leer \"Bés,\" te mu tudde lëndëm \"Guddi.\" Te ngoon ak suba def na bés bu jëkk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "i did this manually by hand below (implementation after this box). once we\n",
        "are able to get per sentence translation we can use this box\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.translate import bleu_score\n",
        "\n",
        "val_list = list(wolof['passages'].keys())\n",
        "\n",
        "references = []\n",
        "trans_chat = []\n",
        "trans_google = []\n",
        "\n",
        "for val in val_list:\n",
        "  ref = wolof['passages'][val]['official']\n",
        "  sent_ref = sent_tokenize(ref)\n",
        "  for sent in sent_ref:\n",
        "    tokens = word_tokenize(sent)\n",
        "    references.append(tokens)\n",
        "\n",
        "  tran_chat = wolof['passages'][val]['chatgpt']\n",
        "  sent_chat = sent_tokenize(tran_chat)\n",
        "  for sent in sent_chat:\n",
        "    tokens = word_tokenize(sent)\n",
        "    trans_chat.append(tokens)\n",
        "\n",
        "print(references)\n",
        "print(trans_chat)\n",
        "\n",
        "score = bleu_score.corpus_bleu(references, trans_chat)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "HrT_nlUOdRiK",
        "outputId": "3675b708-20e9-42a8-c43e-d56d2c794da1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'wolof' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d01c9c83c2bb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mval_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwolof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'passages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wolof' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.translate import bleu_score\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "ref_1 = \"Ca njàlbéen ga Yàlla sàkk na asamaan ak suuf. Suuf nag daa maase woon te wéet, lëndëm muur ndox mu xóot mi, lale ca kaw, Noowug Yàlla di wër, tiim ndox mi. Ba loolu amee Yàlla ne: «Na leer nekk,» leer daldi am. Yàlla gis ne leer gi lu baax la, mu xàjjale leer ak lëndëm. Yàlla tudde leer gi bëccëg, lëndëm gi guddi. Ngoon jot, suba dugg, muy bés ba jëkk.\"\n",
        "trans_chat_1 = \"Ci kawas, Yàlla sàkk ay asamaan ak suuf. Suuf bi dootul am benn doxaan, te nekkul dara; te lëndëm taxawoon na ci kaw géej gu réy. Te Xelmu Yàlla dafa yéegoon ci kaw ndox yi. Te Yàlla wax ne: \\\"Na leer am.\\\" Leer am na. Te Yàlla gis leer, ne baax na; mu deal leer ak lëndëm. Te Yàlla tudde leer \\\"Bés,\\\" te mu tudde lëndëm \\\"Guddi.\\\" Te ngoon ak suba def na bés bu jëkk.\"\n",
        "trans_google_1 = \"Ca njàlbéen ga Yàlla sàkk na asamaan ak suuf. Suuf si amul jemm, neen; Lëndëm muur géej gu xóot gi. Noonu Xelum Yàlla dafa doon dawal ci kaw ndox mi. Yàlla ne: «Na leer am.» Yàlla gis ne, leer gi lu baax la, mu xàjjale leer ak lëndëm. Yàlla tudde leer gi bëccëg, lëndëm gi guddi. Ngoon jot, suba dugg, muy bés bu jëkk ba.\"\n",
        "\n",
        "references = []\n",
        "chat_sent = []\n",
        "google_sent = []\n",
        "\n",
        "\n",
        "sentences = []\n",
        "sent_ref_1 = sent_tokenize(ref_1)\n",
        "#print(len(sent_ref_1))\n",
        "#print(sent_ref_1)\n",
        "for sent in sent_ref_1:\n",
        "  tokens = word_tokenize(sent)\n",
        "  sentences.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "for sent in sentences:\n",
        "  sent_1 = []\n",
        "  sent_1.append(sent)\n",
        "  references.append(sent)\n",
        "\n",
        "chat_1 = []\n",
        "sent_chat_1 = sent_tokenize(trans_chat_1)\n",
        "#print(len(sent_chat_1))\n",
        "#print(sent_chat_1)\n",
        "for sent in sent_chat_1:\n",
        "  tokens = word_tokenize(sent)\n",
        "  chat_1.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "sent_1 = []\n",
        "sent_1.append(chat_1[0])\n",
        "chat_sent.append(sent_1)\n",
        "\n",
        "sent_2 = []\n",
        "sent_2.append(chat_1[1])\n",
        "chat_sent.append(sent_2)\n",
        "\n",
        "sent_3 = []\n",
        "sent_3.append(chat_1[2])\n",
        "sent_3.append(chat_1[3])\n",
        "sent_3.append(chat_1[4])\n",
        "chat_sent.append(sent_3)\n",
        "\n",
        "sent_4 = []\n",
        "sent_4.append(chat_1[5])\n",
        "chat_sent.append(sent_4)\n",
        "\n",
        "sent_5 = []\n",
        "sent_5.append(chat_1[6])\n",
        "chat_sent.append(sent_5)\n",
        "\n",
        "sent_6 = []\n",
        "sent_6.append(chat_1[7])\n",
        "chat_sent.append(sent_6)\n",
        "\n",
        "\n",
        "google_1 = []\n",
        "sent_google_1 = sent_tokenize(trans_google_1)\n",
        "#print(len(sent_google_1))\n",
        "#print(sent_google_1)\n",
        "for sent in sent_google_1:\n",
        "  tokens = word_tokenize(sent)\n",
        "  google_1.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "for sent in google_1:\n",
        "  #sent_1 = []\n",
        "  #sent_1.append(sent)\n",
        "  google_sent.append(sent)\n",
        "\n",
        "\n",
        "\n",
        "#print(references)\n",
        "#print(chat_sent)\n",
        "#print(google_sent)\n",
        "\n",
        "chat_score = bleu_score.corpus_bleu(chat_sent, references)\n",
        "print(\"ChatGPT BLUE score for passage 1:\", chat_score)\n",
        "\n",
        "google_score = bleu_score.corpus_bleu(references, google_sent, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"Google Translate BLEU score for passage 1\", google_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHbvBq6HhakT",
        "outputId": "0857e6e2-f3fc-4c70-f252-1bcd844b62e4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT BLUE score for passage 1: 0.10716668160929929\n",
            "Google Translate BLEU score for passage 1 0.024538942541568833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ref_2 = \" Aji Sax jee may sàmm, du lenn lu ma ñàkk. Parlu mu naat la may gooral, wal mu dal, mu tette ma ca, leqli ma, teg ma ci yooni njekk ngir teddngay turam. Su may jaare xuru ndee wu ne këruus it, duma ragal loraange: yaw laak man, sa bantu sàmm ak sa nguul, yooyoo may dalal. Yaa ma berndeel, noon yi gis, nga diwal ma saa bopp, terale ma, saab kaas rembat. Mbaax ak ngor kay la ma Aji Sax jiy dare sama giiru dund, ma dëkke këram ba fàww.\"\n",
        "trans_chat_2 = \"Boroom bi mooy sama yar; doo génné ma dara. Mu jox ma yoonu nëbbu ci geej gu ndaw: Mu yóbbu ma ci ndox yu dal. Mu wéccale sama xol: Mu yóbbu ma ci yoon yu jub ngir turam. Bu ma yokk seen yoon ci diiwaanu lëndëm gu dee, Duma ragal dara, ndaxte yaw nga ma wax: Sa yëkkëti yi ak sa sandu yi di ma ñor. Nga wéyal la bànk bi ci kanamu sama noon: Nga taxawe sama bopp ak diw: Sama beer màndal. Na dëgg ci jàmm ak yërmande di ma topp. Jant bi yépp ci sama dund: Te dinaa dëkk ci kër Boroom bi ba fàww.\"\n",
        "trans_google_2 = \"Aji Sax ji mooy sama sàmm; Duma soxla dara. Daf may tëral ci tool yu naat, di yóbbu ma ci wetu ndox mu dal. Dafay féexal sama xol, daf may yóbbu ci yooni njub, ndax turam. Waaw, doonte damay dox ci lëndëm gi lëndëm dee, duma ragal dara, ndax yaa ngi ànd ak man. Sa yet ak sa yet ñoo may féexal xol. Yaa ngi may defaral taabal ci samay noon, yaa ngi diw sama bopp diw; Sama kaas a ngi fees dell. Ci dëgg, mbaax ak yërmaande dina ñu ma topp ci sama dundu, te dinaa dëkk ci kër Yàlla ba fàww.\"\n",
        "\n",
        "references = []\n",
        "chat_sent = []\n",
        "google_sent = []\n",
        "\n",
        "\n",
        "sentences = []\n",
        "sent_ref_2 = sent_tokenize(ref_2)\n",
        "#print(len(sent_ref_2))\n",
        "#print(sent_ref_1)\n",
        "for sent in sent_ref_2:\n",
        "  tokens = word_tokenize(sent)\n",
        "  sentences.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "for sent in sentences:\n",
        "  #sent_1 = []\n",
        "  #sent_1.append(sent)\n",
        "  references.append(sent)\n",
        "\n",
        "chat_2 = []\n",
        "sent_chat_2 = sent_tokenize(trans_chat_2)\n",
        "#print(len(sent_chat_2))\n",
        "#print(sent_chat_1)\n",
        "for sent in sent_chat_2:\n",
        "  tokens = word_tokenize(sent)\n",
        "  chat_2.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "sent_1 = []\n",
        "sent_1.append(chat_2[0])\n",
        "chat_sent.append(sent_1)\n",
        "\n",
        "sent_2 = []\n",
        "sent_2.append(chat_2[1])\n",
        "sent_2.append(chat_2[2])\n",
        "chat_sent.append(sent_2)\n",
        "\n",
        "sent_3 = []\n",
        "sent_3.append(chat_1[3])\n",
        "chat_sent.append(sent_3)\n",
        "\n",
        "sent_4 = []\n",
        "sent_4.append(chat_1[4])\n",
        "sent_4.append(chat_1[5])\n",
        "chat_sent.append(sent_4)\n",
        "\n",
        "sent_5 = []\n",
        "sent_5.append(chat_1[6])\n",
        "chat_sent.append(sent_5)\n",
        "\n",
        "google_2 = []\n",
        "sent_google_2 = sent_tokenize(trans_google_2)\n",
        "#print(len(sent_google_2))\n",
        "#print(sent_google_1)\n",
        "for sent in sent_google_2:\n",
        "  tokens = word_tokenize(sent)\n",
        "  google_2.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "sent_1 = []\n",
        "sent_1.append(google_2[0])\n",
        "google_sent.append(sent_1)\n",
        "\n",
        "sent_2 = []\n",
        "sent_2.append(google_2[1])\n",
        "sent_2.append(google_2[2])\n",
        "google_sent.append(sent_2)\n",
        "\n",
        "sent_3 = []\n",
        "sent_3.append(google_2[3])\n",
        "google_sent.append(sent_3)\n",
        "\n",
        "sent_4 = []\n",
        "sent_4.append(google_2[4])\n",
        "sent_4.append(google_2[5])\n",
        "google_sent.append(sent_4)\n",
        "\n",
        "sent_5 = []\n",
        "sent_5.append(google_2[6])\n",
        "google_sent.append(sent_5)\n",
        "\n",
        "chat_score = bleu_score.corpus_bleu(chat_sent, references, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"ChatGPT BLEU score for passage 2:\", chat_score)\n",
        "\n",
        "google_score = bleu_score.corpus_bleu(google_sent, references, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"Google Translate BLEU score for passage 2:\", google_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab6b0d9-098e-495e-e8aa-765fe03cef19",
        "id": "Ea7rGJc4yvU3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT BLEU score for passage 2: 0.02591610642484559\n",
            "Google Translate BLEU score for passage 2: 0.055594669088597955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ref_3 = \"Tuberkuloos dafay tasaaroo ci ngelaw li, suede tuberkuloos bu tar moo la jàpp, soo sëqëtee wala nga tifli wala nga wax, wala nga tissooli mën nga ko wàlle. Nit ñi feebar bi di nelaw ci seen yaram du ñu wàlle feebar bi. Nit ñi gëna mëna wàlle feebar bi mooy ñi am VIH/SIDA ak tóxkat yi. Ngir saytu jàngoro ju tar ji ci yaram nit, dañuy rajo dënn bi, dañuy jël itam ndox mu bawoo ci moom ñu xool ko ci microscope wala ñu amal ci njàngat mu xóot. Sudee tuberkuloos biy nelaw nak, ci deru nit ki wala ci deretam.\"\n",
        "trans_chat_3 = \"Tuberculosis di faatax ci asamaan bu nit ñi am TB bu dox ci seen yaram di xaax, tufli, wax, walla di ndofli. Nit ñi am TB bu bëggadi dox duñu faatax jàngoro bi. Jàngoro bi di doxati lu gëna bari ci nit ñi am HIV/AIDS ak ñi di naww tabak. Ngir xam TB bu dox, ñu di gis ci X-rayu loxo ak seetlu ci microscope ak culture ci liquides du yaram. Ngir xam TB bu bëggadi dox, ñu di jëfandikoo test tuberculin ci loxo (TST) walla test ci deret.\"\n",
        "trans_google_3 = \"Tuberculose dafay tasaaroo ci air bi sudee nit ñi am TB bu am doole ci seeni poumon dañu sëqët, tufli, wax wala tëf. Ñi am TB bu nëbbu duñu tasaare feebar bi. Infeksioŋ bi dafay gëna faral di am ci ñi am VIH/SIDA ak ñiy tóx. Liy tax ñu xam ndax TB la am mingi aju ci rayon X yi ci dënn bi, ak ci saytu microscope ak ci culture ci ndoxu yaram wi. Soo bëggee xam ndax amna TB bu nëbbu, dangay jël test der bu tuberculin (TST) wala test deret.\"\n",
        "\n",
        "references = []\n",
        "chat_sent = []\n",
        "google_sent = []\n",
        "\n",
        "\n",
        "sentences = []\n",
        "sent_ref_3 = sent_tokenize(ref_3)\n",
        "#print(len(sent_ref_3))\n",
        "#print(sent_ref_1)\n",
        "for sent in sent_ref_3:\n",
        "  tokens = word_tokenize(sent)\n",
        "  sentences.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "for sent in sentences:\n",
        "  sent_1 = []\n",
        "  sent_1.append(sent)\n",
        "  references.append(sent)\n",
        "\n",
        "chat_3 = []\n",
        "sent_chat_3 = sent_tokenize(trans_chat_3)\n",
        "#print(len(sent_chat_3))\n",
        "#print(sent_chat_1)\n",
        "for sent in sent_chat_3:\n",
        "  tokens = word_tokenize(sent)\n",
        "  chat_3.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "for sent in chat_3:\n",
        "  sent_1 = []\n",
        "  sent_1.append(sent)\n",
        "  chat_sent.append(sent)\n",
        "\n",
        "\n",
        "google_3 = []\n",
        "sent_google_3 = sent_tokenize(trans_google_3)\n",
        "#print(len(sent_google_3))\n",
        "#print(sent_google_1)\n",
        "for sent in sent_google_3:\n",
        "  tokens = word_tokenize(sent)\n",
        "  google_3.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "for sent in google_3:\n",
        "  sent_1 = []\n",
        "  sent_1.append(sent)\n",
        "  google_sent.append(sent)\n",
        "\n",
        "\n",
        "chat_score = bleu_score.corpus_bleu(references, chat_sent, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"ChatGPT BLEU score for passage 3:\", chat_score)\n",
        "\n",
        "google_score = bleu_score.corpus_bleu(references, google_sent, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"Google Translate BLEU score for passage 3:\", google_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a82daa2-f460-41a3-e03e-f0be2b93603e",
        "id": "MC5KVQP-BaUu"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT BLEU score for passage 3: 0.017871970945748855\n",
            "Google Translate BLEU score for passage 3: 0.017499471260905855\n"
          ]
        }
      ]
    }
  ]
}