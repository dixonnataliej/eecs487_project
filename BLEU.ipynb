{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ca njàlbéen ga Yàlla sàkk na asamaan ak suuf. Suuf nag daa maase woon te wéet, lëndëm muur ndox mu xóot mi, lale ca kaw, Noowug Yàlla di wër, tiim ndox mi.Ba loolu amee Yàlla ne: «Na leer nekk,» leer daldi am. Yàlla gis ne leer gi lu baax la, mu xàjjale leer ak lëndëm. Yàlla tudde leer gi bëccëg, lëndëm gi guddi. Ngoon jot, suba dugg, muy bés ba jëkk.\n",
            "Ci kawas, Yàlla sàkk ay asamaan ak suuf. Suuf bi dootul am benn doxaan, te nekkul dara; te lëndëm taxawoon na ci kaw géej gu réy. Te Xelmu Yàlla dafa yéegoon ci kaw ndox yi. Te Yàlla wax ne: \"Na leer am.\" Leer am na. Te Yàlla gis leer, ne baax na; mu deal leer ak lëndëm. Te Yàlla tudde leer \"Bés,\" te mu tudde lëndëm \"Guddi.\" Te ngoon ak suba def na bés bu jëkk.\n"
          ]
        }
      ],
      "source": [
        "with open('wolof.json', 'r') as fp:\n",
        "    wolof = json.load(fp)\n",
        "print(wolof['passages']['1']['official'])\n",
        "print(wolof['passages']['1']['chatgpt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'wolof' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
            "\u001b[0;32m<ipython-input-2-d01c9c83c2bb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
            "\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m----> 4\u001b[0;31m \u001b[0mval_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwolof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'passages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m      6\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wolof' is not defined"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "i did this manually by hand below (implementation after this box). once we\n",
        "are able to get per sentence translation we can use this box\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.translate import bleu_score\n",
        "\n",
        "val_list = list(wolof['passages'].keys())\n",
        "\n",
        "references = []\n",
        "trans_chat = []\n",
        "trans_google = []\n",
        "\n",
        "for val in val_list:\n",
        "  ref = wolof['passages'][val]['official']\n",
        "  sent_ref = sent_tokenize(ref)\n",
        "  for sent in sent_ref:\n",
        "    tokens = word_tokenize(sent)\n",
        "    references.append(tokens)\n",
        "\n",
        "  tran_chat = wolof['passages'][val]['chatgpt']\n",
        "  sent_chat = sent_tokenize(tran_chat)\n",
        "  for sent in sent_chat:\n",
        "    tokens = word_tokenize(sent)\n",
        "    trans_chat.append(tokens)\n",
        "\n",
        "print(references)\n",
        "print(trans_chat)\n",
        "\n",
        "score = bleu_score.corpus_bleu(references, trans_chat)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT BLUE score for passage 1: 0.10716668160929929\n",
            "Google Translate BLEU score for passage 1 0.024538942541568833\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.translate import bleu_score\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "ref_1 = \"Ca njàlbéen ga Yàlla sàkk na asamaan ak suuf. Suuf nag daa maase woon te wéet, lëndëm muur ndox mu xóot mi, lale ca kaw, Noowug Yàlla di wër, tiim ndox mi. Ba loolu amee Yàlla ne: «Na leer nekk,» leer daldi am. Yàlla gis ne leer gi lu baax la, mu xàjjale leer ak lëndëm. Yàlla tudde leer gi bëccëg, lëndëm gi guddi. Ngoon jot, suba dugg, muy bés ba jëkk.\"\n",
        "trans_chat_1 = \"Ci kawas, Yàlla sàkk ay asamaan ak suuf. Suuf bi dootul am benn doxaan, te nekkul dara; te lëndëm taxawoon na ci kaw géej gu réy. Te Xelmu Yàlla dafa yéegoon ci kaw ndox yi. Te Yàlla wax ne: \\\"Na leer am.\\\" Leer am na. Te Yàlla gis leer, ne baax na; mu deal leer ak lëndëm. Te Yàlla tudde leer \\\"Bés,\\\" te mu tudde lëndëm \\\"Guddi.\\\" Te ngoon ak suba def na bés bu jëkk.\"\n",
        "trans_google_1 = \"Ca njàlbéen ga Yàlla sàkk na asamaan ak suuf. Suuf si amul jemm, neen; Lëndëm muur géej gu xóot gi. Noonu Xelum Yàlla dafa doon dawal ci kaw ndox mi. Yàlla ne: «Na leer am.» Yàlla gis ne, leer gi lu baax la, mu xàjjale leer ak lëndëm. Yàlla tudde leer gi bëccëg, lëndëm gi guddi. Ngoon jot, suba dugg, muy bés bu jëkk ba.\"\n",
        "\n",
        "references = []\n",
        "chat_sent = []\n",
        "google_sent = []\n",
        "\n",
        "\n",
        "sentences = []\n",
        "sent_ref_1 = sent_tokenize(ref_1)\n",
        "#print(len(sent_ref_1))\n",
        "#print(sent_ref_1)\n",
        "for sent in sent_ref_1:\n",
        "  tokens = word_tokenize(sent)\n",
        "  sentences.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "for sent in sentences:\n",
        "  sent_1 = []\n",
        "  sent_1.append(sent)\n",
        "  references.append(sent)\n",
        "\n",
        "chat_1 = []\n",
        "sent_chat_1 = sent_tokenize(trans_chat_1)\n",
        "#print(len(sent_chat_1))\n",
        "#print(sent_chat_1)\n",
        "for sent in sent_chat_1:\n",
        "  tokens = word_tokenize(sent)\n",
        "  chat_1.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "sent_1 = []\n",
        "sent_1.append(chat_1[0])\n",
        "chat_sent.append(sent_1)\n",
        "\n",
        "sent_2 = []\n",
        "sent_2.append(chat_1[1])\n",
        "chat_sent.append(sent_2)\n",
        "\n",
        "sent_3 = []\n",
        "sent_3.append(chat_1[2])\n",
        "sent_3.append(chat_1[3])\n",
        "sent_3.append(chat_1[4])\n",
        "chat_sent.append(sent_3)\n",
        "\n",
        "sent_4 = []\n",
        "sent_4.append(chat_1[5])\n",
        "chat_sent.append(sent_4)\n",
        "\n",
        "sent_5 = []\n",
        "sent_5.append(chat_1[6])\n",
        "chat_sent.append(sent_5)\n",
        "\n",
        "sent_6 = []\n",
        "sent_6.append(chat_1[7])\n",
        "chat_sent.append(sent_6)\n",
        "\n",
        "\n",
        "google_1 = []\n",
        "sent_google_1 = sent_tokenize(trans_google_1)\n",
        "#print(len(sent_google_1))\n",
        "#print(sent_google_1)\n",
        "for sent in sent_google_1:\n",
        "  tokens = word_tokenize(sent)\n",
        "  google_1.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "for sent in google_1:\n",
        "  #sent_1 = []\n",
        "  #sent_1.append(sent)\n",
        "  google_sent.append(sent)\n",
        "\n",
        "\n",
        "\n",
        "#print(references)\n",
        "#print(chat_sent)\n",
        "#print(google_sent)\n",
        "\n",
        "chat_score = bleu_score.corpus_bleu(chat_sent, references)\n",
        "print(\"ChatGPT BLUE score for passage 1:\", chat_score)\n",
        "\n",
        "google_score = bleu_score.corpus_bleu(references, google_sent, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"Google Translate BLEU score for passage 1\", google_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT BLEU score for passage 2: 0.02591610642484559\n",
            "Google Translate BLEU score for passage 2: 0.055594669088597955\n"
          ]
        }
      ],
      "source": [
        "ref_2 = \" Aji Sax jee may sàmm, du lenn lu ma ñàkk. Parlu mu naat la may gooral, wal mu dal, mu tette ma ca, leqli ma, teg ma ci yooni njekk ngir teddngay turam. Su may jaare xuru ndee wu ne këruus it, duma ragal loraange: yaw laak man, sa bantu sàmm ak sa nguul, yooyoo may dalal. Yaa ma berndeel, noon yi gis, nga diwal ma saa bopp, terale ma, saab kaas rembat. Mbaax ak ngor kay la ma Aji Sax jiy dare sama giiru dund, ma dëkke këram ba fàww.\"\n",
        "trans_chat_2 = \"Boroom bi mooy sama yar; doo génné ma dara. Mu jox ma yoonu nëbbu ci geej gu ndaw: Mu yóbbu ma ci ndox yu dal. Mu wéccale sama xol: Mu yóbbu ma ci yoon yu jub ngir turam. Bu ma yokk seen yoon ci diiwaanu lëndëm gu dee, Duma ragal dara, ndaxte yaw nga ma wax: Sa yëkkëti yi ak sa sandu yi di ma ñor. Nga wéyal la bànk bi ci kanamu sama noon: Nga taxawe sama bopp ak diw: Sama beer màndal. Na dëgg ci jàmm ak yërmande di ma topp. Jant bi yépp ci sama dund: Te dinaa dëkk ci kër Boroom bi ba fàww.\"\n",
        "trans_google_2 = \"Aji Sax ji mooy sama sàmm; Duma soxla dara. Daf may tëral ci tool yu naat, di yóbbu ma ci wetu ndox mu dal. Dafay féexal sama xol, daf may yóbbu ci yooni njub, ndax turam. Waaw, doonte damay dox ci lëndëm gi lëndëm dee, duma ragal dara, ndax yaa ngi ànd ak man. Sa yet ak sa yet ñoo may féexal xol. Yaa ngi may defaral taabal ci samay noon, yaa ngi diw sama bopp diw; Sama kaas a ngi fees dell. Ci dëgg, mbaax ak yërmaande dina ñu ma topp ci sama dundu, te dinaa dëkk ci kër Yàlla ba fàww.\"\n",
        "\n",
        "references = []\n",
        "chat_sent = []\n",
        "google_sent = []\n",
        "\n",
        "\n",
        "sentences = []\n",
        "sent_ref_2 = sent_tokenize(ref_2)\n",
        "#print(len(sent_ref_2))\n",
        "#print(sent_ref_1)\n",
        "for sent in sent_ref_2:\n",
        "  tokens = word_tokenize(sent)\n",
        "  sentences.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "for sent in sentences:\n",
        "  #sent_1 = []\n",
        "  #sent_1.append(sent)\n",
        "  references.append(sent)\n",
        "\n",
        "chat_2 = []\n",
        "sent_chat_2 = sent_tokenize(trans_chat_2)\n",
        "#print(len(sent_chat_2))\n",
        "#print(sent_chat_1)\n",
        "for sent in sent_chat_2:\n",
        "  tokens = word_tokenize(sent)\n",
        "  chat_2.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "sent_1 = []\n",
        "sent_1.append(chat_2[0])\n",
        "chat_sent.append(sent_1)\n",
        "\n",
        "sent_2 = []\n",
        "sent_2.append(chat_2[1])\n",
        "sent_2.append(chat_2[2])\n",
        "chat_sent.append(sent_2)\n",
        "\n",
        "sent_3 = []\n",
        "sent_3.append(chat_1[3])\n",
        "chat_sent.append(sent_3)\n",
        "\n",
        "sent_4 = []\n",
        "sent_4.append(chat_1[4])\n",
        "sent_4.append(chat_1[5])\n",
        "chat_sent.append(sent_4)\n",
        "\n",
        "sent_5 = []\n",
        "sent_5.append(chat_1[6])\n",
        "chat_sent.append(sent_5)\n",
        "\n",
        "google_2 = []\n",
        "sent_google_2 = sent_tokenize(trans_google_2)\n",
        "#print(len(sent_google_2))\n",
        "#print(sent_google_1)\n",
        "for sent in sent_google_2:\n",
        "  tokens = word_tokenize(sent)\n",
        "  google_2.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "sent_1 = []\n",
        "sent_1.append(google_2[0])\n",
        "google_sent.append(sent_1)\n",
        "\n",
        "sent_2 = []\n",
        "sent_2.append(google_2[1])\n",
        "sent_2.append(google_2[2])\n",
        "google_sent.append(sent_2)\n",
        "\n",
        "sent_3 = []\n",
        "sent_3.append(google_2[3])\n",
        "google_sent.append(sent_3)\n",
        "\n",
        "sent_4 = []\n",
        "sent_4.append(google_2[4])\n",
        "sent_4.append(google_2[5])\n",
        "google_sent.append(sent_4)\n",
        "\n",
        "sent_5 = []\n",
        "sent_5.append(google_2[6])\n",
        "google_sent.append(sent_5)\n",
        "\n",
        "chat_score = bleu_score.corpus_bleu(chat_sent, references, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"ChatGPT BLEU score for passage 2:\", chat_score)\n",
        "\n",
        "google_score = bleu_score.corpus_bleu(google_sent, references, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"Google Translate BLEU score for passage 2:\", google_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT BLEU score for passage 3: 0.017871970945748855\n",
            "Google Translate BLEU score for passage 3: 0.017499471260905855\n"
          ]
        }
      ],
      "source": [
        "ref_3 = \"Tuberkuloos dafay tasaaroo ci ngelaw li, suede tuberkuloos bu tar moo la jàpp, soo sëqëtee wala nga tifli wala nga wax, wala nga tissooli mën nga ko wàlle. Nit ñi feebar bi di nelaw ci seen yaram du ñu wàlle feebar bi. Nit ñi gëna mëna wàlle feebar bi mooy ñi am VIH/SIDA ak tóxkat yi. Ngir saytu jàngoro ju tar ji ci yaram nit, dañuy rajo dënn bi, dañuy jël itam ndox mu bawoo ci moom ñu xool ko ci microscope wala ñu amal ci njàngat mu xóot. Sudee tuberkuloos biy nelaw nak, ci deru nit ki wala ci deretam.\"\n",
        "trans_chat_3 = \"Tuberculosis di faatax ci asamaan bu nit ñi am TB bu dox ci seen yaram di xaax, tufli, wax, walla di ndofli. Nit ñi am TB bu bëggadi dox duñu faatax jàngoro bi. Jàngoro bi di doxati lu gëna bari ci nit ñi am HIV/AIDS ak ñi di naww tabak. Ngir xam TB bu dox, ñu di gis ci X-rayu loxo ak seetlu ci microscope ak culture ci liquides du yaram. Ngir xam TB bu bëggadi dox, ñu di jëfandikoo test tuberculin ci loxo (TST) walla test ci deret.\"\n",
        "trans_google_3 = \"Tuberculose dafay tasaaroo ci air bi sudee nit ñi am TB bu am doole ci seeni poumon dañu sëqët, tufli, wax wala tëf. Ñi am TB bu nëbbu duñu tasaare feebar bi. Infeksioŋ bi dafay gëna faral di am ci ñi am VIH/SIDA ak ñiy tóx. Liy tax ñu xam ndax TB la am mingi aju ci rayon X yi ci dënn bi, ak ci saytu microscope ak ci culture ci ndoxu yaram wi. Soo bëggee xam ndax amna TB bu nëbbu, dangay jël test der bu tuberculin (TST) wala test deret.\"\n",
        "\n",
        "references = []\n",
        "chat_sent = []\n",
        "google_sent = []\n",
        "\n",
        "\n",
        "sentences = []\n",
        "sent_ref_3 = sent_tokenize(ref_3)\n",
        "#print(len(sent_ref_3))\n",
        "#print(sent_ref_1)\n",
        "for sent in sent_ref_3:\n",
        "  tokens = word_tokenize(sent)\n",
        "  sentences.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "for sent in sentences:\n",
        "  sent_1 = []\n",
        "  sent_1.append(sent)\n",
        "  references.append(sent)\n",
        "\n",
        "chat_3 = []\n",
        "sent_chat_3 = sent_tokenize(trans_chat_3)\n",
        "#print(len(sent_chat_3))\n",
        "#print(sent_chat_1)\n",
        "for sent in sent_chat_3:\n",
        "  tokens = word_tokenize(sent)\n",
        "  chat_3.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "for sent in chat_3:\n",
        "  sent_1 = []\n",
        "  sent_1.append(sent)\n",
        "  chat_sent.append(sent)\n",
        "\n",
        "\n",
        "google_3 = []\n",
        "sent_google_3 = sent_tokenize(trans_google_3)\n",
        "#print(len(sent_google_3))\n",
        "#print(sent_google_1)\n",
        "for sent in sent_google_3:\n",
        "  tokens = word_tokenize(sent)\n",
        "  google_3.append(tokens)\n",
        "  #print(tokens)\n",
        "\n",
        "for sent in google_3:\n",
        "  sent_1 = []\n",
        "  sent_1.append(sent)\n",
        "  google_sent.append(sent)\n",
        "\n",
        "\n",
        "chat_score = bleu_score.corpus_bleu(references, chat_sent, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"ChatGPT BLEU score for passage 3:\", chat_score)\n",
        "\n",
        "google_score = bleu_score.corpus_bleu(references, google_sent, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"Google Translate BLEU score for passage 3:\", google_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dengan pengumuman kawalan pergerakan, pelbagai pihak misi diplomatik seperti Amerika Syarikat dan Perancis telah berhenti mengeluarkan visa, manakala India telah melarang rakyat Malaysia untuk melancong ke negaranya. Semua rumah ibadat dan premis perniagaan kecuali pasar raya , pasar awam , kedai runcit dan kedai serbaneka yang menjual barangan keperluan harian.\n",
            "Beberapa saat selepas perintah itu diumumkan di Malaysia, dengan pengumuman kawalan pergerakan, pelbagai misi diplomatik seperti Amerika Syarikat dan Perancis telah menghentikan pengeluaran visa, manakala India telah melarang warganegara Malaysia daripada melancong ke negaranya. Semua tempat ibadat dan premis perniagaan perlu ditutup kecuali pasar raya, pasar awam, kedai runcit dan kedai serbaneka yang menjual keperluan harian\n"
          ]
        }
      ],
      "source": [
        "with open('malay.json', 'r') as fp:\n",
        "    malay = json.load(fp)\n",
        "print(malay['passages']['4']['official'])\n",
        "print(malay['passages']['4']['chatgpt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT BLEU score for passage 1 for language Malay: 0.03551852538064544\n",
            "Google Translate BLEU score for passage 1 for language Malay: 0.03875680679640436\n",
            "ChatGPT METEOR score for passage 1 sentence 1 for Malay language: 0.1293103448275862\n",
            "Google Translate METEOR score for passage 1 sentence 1 for Malay language: 0.1293103448275862\n",
            "ChatGPT METEOR score for passage 1 sentence 2 for Malay language: 0.08108108108108109\n",
            "Google Translate METEOR score for passage 1 sentence 2 for Malay language: 0.08064516129032258\n",
            "ChatGPT METEOR score for passage 1 sentence 3 for Malay language: 0.4179775280898876\n",
            "Google Translate METEOR score for passage 1 sentence 3 for Malay language: 0.41333333333333333\n",
            "ChatGPT METEOR score for passage 1 sentence 4 for Malay language: 0.14438166980539863\n",
            "Google Translate METEOR score for passage 1 sentence 4 for Malay language: 0.08720930232558138\n",
            "ChatGPT METEOR score for passage 1 sentence 5 for Malay language: 0.0574712643678161\n",
            "Google Translate METEOR score for passage 1 sentence 5 for Malay language: 0.1149425287356322\n",
            "ChatGPT METEOR score for passage 1 sentence 6 for Malay language: 0.08196721311475409\n",
            "Google Translate METEOR score for passage 1 sentence 6 for Malay language: 0.10869565217391304\n",
            "ChatGPT METEOR score for passage 1 sentence 7 for Malay language: 0.38490853658536583\n",
            "Google Translate METEOR score for passage 1 sentence 7 for Malay language: 0.3194444444444444\n",
            "ChatGPT METEOR score for passage 1 for Malay language: 0.18529966255312708\n",
            "Google Translate METEOR score for passage 1 for Malay language: 0.17908296673297328\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "BLEU and METEOR scores for Malay language passage 1\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.translate import bleu_score\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "sent_malay_1 = malay['passages']['1']['official']\n",
        "chat_malay_1 = malay['passages']['1']['chatgpt']\n",
        "google_malay_1 = malay['passages']['1']['google']\n",
        "\n",
        "sent_token = sent_tokenize(sent_malay_1)\n",
        "\n",
        "chat_token = sent_tokenize(chat_malay_1)\n",
        "\n",
        "google_token = sent_tokenize(google_malay_1)\n",
        "\n",
        "sent_malay_tokenized = []\n",
        "for sent in sent_token:\n",
        "  tokens = word_tokenize(sent)\n",
        "  sent_malay_tokenized.append(tokens)\n",
        "\n",
        "chat_malay_tokenized = []\n",
        "for sent in chat_token:\n",
        "  tokens = word_tokenize(sent)\n",
        "  chat_malay_tokenized.append(tokens)\n",
        "\n",
        "google_malay_tokenized = []\n",
        "for sent in google_token:\n",
        "  tokens = word_tokenize(sent)\n",
        "  google_malay_tokenized.append(tokens)\n",
        "\n",
        "references = []\n",
        "\n",
        "sent_1 = []\n",
        "sent_1.append(sent_malay_tokenized[0])\n",
        "references.append(sent_1)\n",
        "\n",
        "sent_2 = []\n",
        "sent_2.append(sent_malay_tokenized[1])\n",
        "references.append(sent_2)\n",
        "\n",
        "sent_3 = []\n",
        "sent_3.append(sent_malay_tokenized[2])\n",
        "references.append(sent_3)\n",
        "\n",
        "sent_4 = []\n",
        "sent_4.append(sent_malay_tokenized[3])\n",
        "sent_4.append(sent_malay_tokenized[4])\n",
        "references.append(sent_4)\n",
        "\n",
        "sent_5 = []\n",
        "sent_5.append(sent_malay_tokenized[5])\n",
        "references.append(sent_5)\n",
        "\n",
        "sent_6 = []\n",
        "sent_6.append(sent_malay_tokenized[6])\n",
        "references.append(sent_6)\n",
        "\n",
        "sent_7 = []\n",
        "sent_7.append(sent_malay_tokenized[7])\n",
        "references.append(sent_7)\n",
        "\n",
        "chat_score_bleu = bleu_score.corpus_bleu(references, chat_malay_tokenized, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"ChatGPT BLEU score for passage 1 for language Malay:\", chat_score_bleu)\n",
        "\n",
        "google_score_bleu = bleu_score.corpus_bleu(references, google_malay_tokenized, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"Google Translate BLEU score for passage 1 for language Malay:\", google_score_bleu)\n",
        "\n",
        "\"\"\"\n",
        "meteor scoring, has to be done per sentence\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "avg_chat_meteor = 0.0\n",
        "avg_google_meteor = 0.0\n",
        "\n",
        "chat_meteor_score = meteor_score([sent_malay_tokenized[0]], chat_malay_tokenized[0])\n",
        "avg_chat_meteor += chat_meteor_score\n",
        "print(\"ChatGPT METEOR score for passage 1 sentence 1 for Malay language:\", chat_meteor_score)\n",
        "\n",
        "google_meteor_score = meteor_score([sent_malay_tokenized[0]], google_malay_tokenized[0])\n",
        "avg_google_meteor += google_meteor_score\n",
        "print(\"Google Translate METEOR score for passage 1 sentence 1 for Malay language:\", google_meteor_score)\n",
        "\n",
        "chat_meteor_score = meteor_score([sent_malay_tokenized[1]], chat_malay_tokenized[1])\n",
        "avg_chat_meteor += chat_meteor_score\n",
        "print(\"ChatGPT METEOR score for passage 1 sentence 2 for Malay language:\", chat_meteor_score)\n",
        "\n",
        "google_meteor_score = meteor_score([sent_malay_tokenized[1]], google_malay_tokenized[1])\n",
        "avg_google_meteor += google_meteor_score\n",
        "print(\"Google Translate METEOR score for passage 1 sentence 2 for Malay language:\", google_meteor_score)\n",
        "\n",
        "chat_meteor_score = meteor_score([sent_malay_tokenized[2]], chat_malay_tokenized[2])\n",
        "avg_chat_meteor += chat_meteor_score\n",
        "print(\"ChatGPT METEOR score for passage 1 sentence 3 for Malay language:\", chat_meteor_score)\n",
        "\n",
        "google_meteor_score = meteor_score([sent_malay_tokenized[2]], google_malay_tokenized[2])\n",
        "avg_google_meteor += google_meteor_score\n",
        "print(\"Google Translate METEOR score for passage 1 sentence 3 for Malay language:\", google_meteor_score)\n",
        "\n",
        "chat_meteor_score = meteor_score([sent_malay_tokenized[3]], chat_malay_tokenized[3])\n",
        "avg_chat_meteor += chat_meteor_score\n",
        "print(\"ChatGPT METEOR score for passage 1 sentence 4 for Malay language:\", chat_meteor_score)\n",
        "\n",
        "google_meteor_score = meteor_score([sent_malay_tokenized[3]], google_malay_tokenized[3])\n",
        "avg_google_meteor += google_meteor_score\n",
        "print(\"Google Translate METEOR score for passage 1 sentence 4 for Malay language:\", google_meteor_score)\n",
        "\n",
        "chat_meteor_score = meteor_score([sent_malay_tokenized[4]], chat_malay_tokenized[4])\n",
        "avg_chat_meteor += chat_meteor_score\n",
        "print(\"ChatGPT METEOR score for passage 1 sentence 5 for Malay language:\", chat_meteor_score)\n",
        "\n",
        "google_meteor_score = meteor_score([sent_malay_tokenized[4]], google_malay_tokenized[4])\n",
        "avg_google_meteor += google_meteor_score\n",
        "print(\"Google Translate METEOR score for passage 1 sentence 5 for Malay language:\", google_meteor_score)\n",
        "\n",
        "chat_meteor_score = meteor_score([sent_malay_tokenized[6]], chat_malay_tokenized[5])\n",
        "avg_chat_meteor += chat_meteor_score\n",
        "print(\"ChatGPT METEOR score for passage 1 sentence 6 for Malay language:\", chat_meteor_score)\n",
        "\n",
        "google_meteor_score = meteor_score([sent_malay_tokenized[6]], google_malay_tokenized[5])\n",
        "avg_google_meteor += google_meteor_score\n",
        "print(\"Google Translate METEOR score for passage 1 sentence 6 for Malay language:\", google_meteor_score)\n",
        "\n",
        "chat_meteor_score = meteor_score([sent_malay_tokenized[7]], chat_malay_tokenized[6])\n",
        "avg_chat_meteor += chat_meteor_score\n",
        "print(\"ChatGPT METEOR score for passage 1 sentence 7 for Malay language:\", chat_meteor_score)\n",
        "\n",
        "google_meteor_score = meteor_score([sent_malay_tokenized[7]], google_malay_tokenized[6])\n",
        "avg_google_meteor += google_meteor_score\n",
        "print(\"Google Translate METEOR score for passage 1 sentence 7 for Malay language:\", google_meteor_score)\n",
        "\n",
        "print(\"ChatGPT METEOR score for passage 1 for Malay language:\", avg_chat_meteor/7)\n",
        "print(\"Google Translate METEOR score for passage 1 for Malay language:\", avg_google_meteor/7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT BLEU score for example for language Malay: 0.08237090485818377\n",
            "Google Translate BLEU score for example for language Malay: 0.0787960611563452\n",
            "ChatGPT METEOR score for passage 3 sentence 1 for Malay language: 0.19669760509967682\n",
            "Google Translate METEOR score for passage 3 sentence 1 for Malay language: 0.19669760509967682\n",
            "ChatGPT METEOR score for passage 3 sentence 2 for Malay language: 0.2495515458478422\n",
            "Google Translate METEOR score for passage 3 sentence 2 for Malay language: 0.2425631269456935\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "BLEU and METEOR scores for Malay language passage 3\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.translate import bleu_score\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "sent_malay_3 = malay['passages']['3']['official']\n",
        "chat_malay_3 = malay['passages']['3']['chatgpt']\n",
        "google_malay_3 = malay['passages']['3']['google']\n",
        "\n",
        "sent_token = sent_tokenize(sent_malay_3)\n",
        "\n",
        "chat_token = sent_tokenize(chat_malay_3)\n",
        "\n",
        "google_token = sent_tokenize(google_malay_3)\n",
        "\n",
        "sent_malay_tokenized = []\n",
        "for sent in sent_token:\n",
        "  tokens = word_tokenize(sent)\n",
        "  sent_malay_tokenized.append(tokens)\n",
        "\n",
        "chat_malay_tokenized = []\n",
        "for sent in chat_token:\n",
        "  tokens = word_tokenize(sent)\n",
        "  chat_malay_tokenized.append(tokens)\n",
        "\n",
        "google_malay_tokenized = []\n",
        "for sent in google_token:\n",
        "  tokens = word_tokenize(sent)\n",
        "  google_malay_tokenized.append(tokens)\n",
        "\n",
        "references = []\n",
        "\n",
        "\n",
        "for sent in sent_malay_tokenized:\n",
        "  sent_list = []\n",
        "  sent_list.append(sent)\n",
        "  references.append(sent_list)\n",
        "\n",
        "\n",
        "chat_score_bleu = bleu_score.corpus_bleu(references, chat_malay_tokenized, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"ChatGPT BLEU score for example for language Malay:\", chat_score_bleu)\n",
        "\n",
        "google_score_bleu = bleu_score.corpus_bleu(references, google_malay_tokenized, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"Google Translate BLEU score for example for language Malay:\", google_score_bleu)\n",
        "\n",
        "\"\"\"\n",
        "meteor scoring, has to be done per sentence\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for i, sent in enumerate(sent_malay_tokenized):\n",
        "  references = []\n",
        "  references.append(sent)\n",
        "\n",
        "  chat_meteor_score = meteor_score(references, chat_malay_tokenized[i])\n",
        "  print(f\"ChatGPT METEOR score for passage 3 sentence {i+1} for Malay language:\", chat_meteor_score)\n",
        "\n",
        "  google_meteor_score = meteor_score(references, google_malay_tokenized[i])\n",
        "  print(f\"Google Translate METEOR score for passage 3 sentence {i+1} for Malay language:\", google_meteor_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT BLEU score for example for language Malay: 0.5540826937742469\n",
            "Google Translate BLEU score for example for language Malay: 0.6473403627036354\n",
            "ChatGPT METEOR score for example sentence 1 for Malay language: 0.8084415584415583\n",
            "Google Translate METEOR score for example sentence 1 for Malay language: 0.8453636065763352\n",
            "ChatGPT METEOR score for example sentence 2 for Malay language: 0.8719765684051398\n",
            "Google Translate METEOR score for example sentence 2 for Malay language: 0.9493586839350717\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "BLEU and METEOR scores for Malay language passage 4\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.translate import bleu_score\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "sent_malay_4 = malay['passages']['4']['official']\n",
        "chat_malay_4 = malay['passages']['4']['chatgpt']\n",
        "google_malay_4 = malay['passages']['4']['google']\n",
        "\n",
        "sent_token = sent_tokenize(sent_malay_4)\n",
        "\n",
        "chat_token = sent_tokenize(chat_malay_4)\n",
        "\n",
        "google_token = sent_tokenize(google_malay_4)\n",
        "\n",
        "sent_malay_tokenized = []\n",
        "for sent in sent_token:\n",
        "  tokens = word_tokenize(sent)\n",
        "  sent_malay_tokenized.append(tokens)\n",
        "\n",
        "chat_malay_tokenized = []\n",
        "for sent in chat_token:\n",
        "  tokens = word_tokenize(sent)\n",
        "  chat_malay_tokenized.append(tokens)\n",
        "\n",
        "google_malay_tokenized = []\n",
        "for sent in google_token:\n",
        "  tokens = word_tokenize(sent)\n",
        "  google_malay_tokenized.append(tokens)\n",
        "\n",
        "references = []\n",
        "\n",
        "\n",
        "for sent in sent_malay_tokenized:\n",
        "  sent_list = []\n",
        "  sent_list.append(sent)\n",
        "  references.append(sent_list)\n",
        "\n",
        "\n",
        "chat_score_bleu = bleu_score.corpus_bleu(references, chat_malay_tokenized, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"ChatGPT BLEU score for example for language Malay:\", chat_score_bleu)\n",
        "\n",
        "google_score_bleu = bleu_score.corpus_bleu(references, google_malay_tokenized, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"Google Translate BLEU score for example for language Malay:\", google_score_bleu)\n",
        "\n",
        "\"\"\"\n",
        "meteor scoring, has to be done per sentence\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for i, sent in enumerate(sent_malay_tokenized):\n",
        "  references = []\n",
        "  references.append(sent)\n",
        "\n",
        "  chat_meteor_score = meteor_score(references, chat_malay_tokenized[i])\n",
        "  print(f\"ChatGPT METEOR score for example sentence {i+1} for Malay language:\", chat_meteor_score)\n",
        "\n",
        "  google_meteor_score = meteor_score(references, google_malay_tokenized[i])\n",
        "  print(f\"Google Translate METEOR score for example sentence {i+1} for Malay language:\", google_meteor_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ihcuac Yaotl technotzaz mexicah, Ticanacan temicti tepoztli, Ihuan huelihqui, man, tlalcohcomoni, ihcuac totepoz cueponiz nohuian.\n",
            "Mēxihcah, ka tēmik ilwikak tlayok, chikawatiaj tepoztli wan tepetlakatl, wan tlaltikpak chikonetza kualankayotl ipan tetlayokayotl tletetl.\n"
          ]
        }
      ],
      "source": [
        "with open('nahuatl.json', 'r') as fp:\n",
        "    nahuatl = json.load(fp)\n",
        "print(nahuatl['passages']['4']['official'])\n",
        "print(nahuatl['passages']['4']['chatgpt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT BLEU score for example for language Nahuatl: 0.0739615923411127\n",
            "Google Translate BLEU score for example for language Nahuatl: 0.0519220348247666\n",
            "ChatGPT METEOR score for example sentence 1 for Nahuatl language: 0.11961722488038276\n",
            "Google Translate METEOR score for example sentence 1 for Nahuatl language: 0.09216589861751152\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "BLEU and METEOR scores for Nahuatl language\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.translate import bleu_score\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "sent_nahuatl_4 = nahuatl['passages']['4']['official']\n",
        "chat_nahuatl_4 = nahuatl['passages']['4']['chatgpt']\n",
        "google_nahuatl_4 = nahuatl['passages']['4']['google']\n",
        "\n",
        "sent_token = sent_tokenize(sent_nahuatl_4)\n",
        "\n",
        "chat_token = sent_tokenize(chat_nahuatl_4)\n",
        "\n",
        "google_token = sent_tokenize(google_nahuatl_4)\n",
        "\n",
        "sent_nahuatl_tokenized = []\n",
        "for sent in sent_token:\n",
        "  tokens = word_tokenize(sent)\n",
        "  sent_nahuatl_tokenized.append(tokens)\n",
        "\n",
        "chat_nahuatl_tokenized = []\n",
        "for sent in chat_token:\n",
        "  tokens = word_tokenize(sent)\n",
        "  chat_nahuatl_tokenized.append(tokens)\n",
        "\n",
        "google_nahuatl_tokenized = []\n",
        "for sent in google_token:\n",
        "  tokens = word_tokenize(sent)\n",
        "  google_nahuatl_tokenized.append(tokens)\n",
        "\n",
        "references = []\n",
        "\n",
        "\n",
        "for sent in sent_nahuatl_tokenized:\n",
        "  sent_list = []\n",
        "  sent_list.append(sent)\n",
        "  references.append(sent_list)\n",
        "\n",
        "\n",
        "chat_score_bleu = bleu_score.corpus_bleu(references, chat_nahuatl_tokenized, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"ChatGPT BLEU score for example for language Nahuatl:\", chat_score_bleu)\n",
        "\n",
        "google_score_bleu = bleu_score.corpus_bleu(references, google_nahuatl_tokenized, smoothing_function=SmoothingFunction().method2)\n",
        "print(\"Google Translate BLEU score for example for language Nahuatl:\", google_score_bleu)\n",
        "\n",
        "\"\"\"\n",
        "meteor scoring, has to be done per sentence\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for i, sent in enumerate(sent_nahuatl_tokenized):\n",
        "  references = []\n",
        "  references.append(sent)\n",
        "\n",
        "  chat_meteor_score = meteor_score(references, chat_nahuatl_tokenized[i])\n",
        "  print(f\"ChatGPT METEOR score for example sentence {i+1} for Nahuatl language:\", chat_meteor_score)\n",
        "\n",
        "  google_meteor_score = meteor_score(references, google_nahuatl_tokenized[i])\n",
        "  print(f\"Google Translate METEOR score for example sentence {i+1} for Nahuatl language:\", google_meteor_score)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
